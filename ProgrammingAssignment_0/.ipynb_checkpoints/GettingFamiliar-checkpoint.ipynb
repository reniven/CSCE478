{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUPYTER NOTEBOOK TIPS\n",
    "\n",
    "Each rectangular box is called a cell. \n",
    "* Ctrl+ENTER evaluates the current cell; if it contains Python code, it runs the code, if it contains Markdown, it returns rendered text.\n",
    "* Alt+ENTER evaluates the current cell and adds a new cell below it.\n",
    "* If you click to the left of a cell, you'll notice the frame changes color to blue. You can erase a cell by hitting 'dd' (that's two **d**'s in a row) when the frame is blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADING\n",
    "\n",
    "You will be graded on parts that are marked with **TODO** comments. Read the comments in the code to make sure you don't miss any.\n",
    "\n",
    "### Mandatory for 478 & 878:\n",
    "\n",
    "|   | Tasks                      | 478 | 878 |\n",
    "|---|----------------------------|-----|-----|\n",
    "| 1 | Implement `preprocess`     |  10 |   5 |\n",
    "| 2 | Implement `partition`      |  10 |   5 |\n",
    "| 3 | Putting the model together |   5 |   5 |\n",
    "\n",
    "### Mandatory for 878, bonus for 478\n",
    "\n",
    "|   | Tasks                                 | 478 | 878 |\n",
    "|---|---------------------------------------|-----|-----|\n",
    "|4  | Implement `normalization` | 5  | 10   |\n",
    "\n",
    "\n",
    "Points are broken down further below in Rubric sections. The **first** score is for 478, the **second** is for 878 students. There a total of 25 points in this assignment and extra 5 bonus points for 478 students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR GRADE\n",
    "\n",
    "\n",
    "### Group members: Eric Le, Duc Le\n",
    "\n",
    "|   | Tasks                      | Points|\n",
    "|---|----------------------------|-----|\n",
    "| 1 | Implement `preprocess`     |     |\n",
    "| 2 | Implement `partition`      |     |\n",
    "| 3 | Putting the model together |     |\n",
    "|4  | Implement `normalization`  |     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Model Skeleton\n",
    "\n",
    "We'll use this skeleton for implementing different supervised learning algorithms. For this first assignment, we'll read and partition the [**madelon** dataset](http://archive.ics.uci.edu/ml/datasets/madelon). Features and labels for the first two examples are listed below. Please complete **preprocess** and **partition** functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 500 features in the **madelon** dataset have integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nl' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! echo '../data/madelon.data'; head -n 2 ../data/madelon.data | nl -s '-) '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are either positive (1) or negative (-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nl' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! echo '../data/madelon.labels'; head -n 2 ../data/madelon.labels | nl -s '-) '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Implement `preprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is for reading the dataset and for extracting features and labels. The **preprocess** function should return an *n x d* **features** array, and an *n x 1* **labels** array, where *n* is the number of examples and *d* is the number of features in the dataset. In cases where there is a big difference between the scales of features, we want to normalize the features to have values in the same range [0,1]. Since this is not the case with this dataset, we will not do normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocess(feature_file, label_file):\n",
    "    '''\n",
    "    Args:\n",
    "        feature_file: str \n",
    "            file containing features\n",
    "        label_file: str\n",
    "            file containing labels\n",
    "    Returns:\n",
    "        features: ndarray\n",
    "            nxd features\n",
    "        labels: ndarray\n",
    "            nx1 labels\n",
    "    '''\n",
    "    # You might find np.genfromtxt useful for reading in the file. Be careful with the file delimiter, \n",
    "    # e.g. for comma-separated files use delimiter=',' argument.\n",
    "    \n",
    "    # TODO \n",
    "    features = np.genfromtxt(feature_file, delimiter = ' ')\n",
    "    labels = np.genfromtxt(label_file,)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "* Correct features size +5, +2.5\n",
    "* Correct labels size  +5, +2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `preprocess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 500)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "features, labels = preprocess(feature_file = '../data/madelon.data', label_file = '../data/madelon.labels')\n",
    "# TODO: Output the dimension of both features and labels.\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Implement `partition`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll need to split your dataset into training, validation and test sets. The **partition** function should take as input the size of the whole dataset and randomly sample a proportion *t* of the dataset indices for test partition and a proportion of *v* for validation partition. The remaining will be used as indices for training data. For example, to keep 30% of the examples as test and %10 as validation, set *t* = 0.3 and *v* = 0.1. You should choose these values according to the size of the data available to you. The **split** function should return indices of the training, validation and test sets. These will be used to index into the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition(size, t, v = 0):\n",
    "    '''\n",
    "    Args:\n",
    "        size: int\n",
    "            number of examples in the whole dataset\n",
    "        t: float\n",
    "            proportion kept for test\n",
    "        v: float\n",
    "            proportion kept for validation\n",
    "    Returns:\n",
    "        test_indices: ndarray\n",
    "            1D array containing test set indices\n",
    "        val_indices: ndarray\n",
    "            1D array containing validation set indices\n",
    "        train_indices: ndarray\n",
    "            1D array containing train set indices\n",
    "    '''\n",
    "    \n",
    "    # np.random.permutation might come in handy. Do not sample with replacement!\n",
    "    # Be sure not to use the same indices in test and validation sets!\n",
    "    sizeArray = np.random.permutation(size)\n",
    "    # use the first np.ceil(size*t) for test, \n",
    "    # the following np.ceil(size*v) for validation set.\n",
    "    testCeiling = int(np.ceil(size*t))\n",
    "    valCeiling = testCeiling + int(np.ceil(size*v))\n",
    "    test_indices = np.array(sizeArray[0:testCeiling])\n",
    "    val_indices = np.array(sizeArray[testCeiling:valCeiling])\n",
    "    train_indices = np.array(sizeArray[valCeiling:size])\n",
    "    # TODO\n",
    "    \n",
    "    return test_indices, val_indices, train_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "* Correct length of test indices +5, +2.5\n",
    "* Correct length of validation indices +5, +2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `partition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Pass the correct size argument (number of examples in the whole dataset)\n",
    "test_indices, val_indices, train_indices = partition(size=len(features), t = 0.3, v = 0.1)\n",
    "# Output the size of length of test and validation indices.\n",
    "print(len(test_indices))\n",
    "print(len(val_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Putting things together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model definition is given below. We'll extend this class for different supervised classification algorithms. Specifically, we'll implement **fit** and **predict** methods for these algorithms. For this assignment, you are not asked to implement these methods. Run the cells below and make sure each piece of code fits together and works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "        \n",
    "    def fit(self, training_features, training_labels):\n",
    "        print('There are {} data points in training partition with {} features.'.format(\n",
    "            training_features.shape[0], training_features.shape[1]))\n",
    "        return\n",
    "    \n",
    "    def predict(self, test_points):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "* Correct training size +5, +5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `Model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and call fit method with the training features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1200 data points in training partition with 500 features.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# initialize model\n",
    "my_model = Model()\n",
    "# obtain features and labels from files\n",
    "features, labels = preprocess(feature_file = '../data/madelon.data', label_file = '../data/madelon.labels')\n",
    "# partition the data set\n",
    "features_test_indices, features_val_indices, features_train_indices = partition(size=len(features), t = 0.3, v = 0.1)\n",
    "labels_test_indices, labels_val_indices, label_train_indices = partition(size=len(labels), t = 0.3, v = 0.1)\n",
    "training_features = []\n",
    "\n",
    "for z in features_train_indices:\n",
    "    training_features.append(features[z])\n",
    "\n",
    "training_labels = []\n",
    "\n",
    "for x in label_train_indices:\n",
    "    training_labels.append(labels[x])\n",
    "    \n",
    "training_features = np.array(training_features)\n",
    "training_labels = np.array(training_labels)\n",
    "# pass the training features and labels to the fit method\n",
    "my_model.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `normalization` function such that the output features take values in the range [0, 1]. Check that the values of the features are in [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "* Correct range for feature values +5, +10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization(raw_features):\n",
    "    '''\n",
    "    Args:\n",
    "        raw_features: ndarray\n",
    "            nxd array containing unnormalized features\n",
    "    Returns:\n",
    "        features: ndarray\n",
    "            nxd array containing normalized features\n",
    "            \n",
    "    '''\n",
    "    raw_features_max, raw_features_min = raw_features.max(), raw_features.min()\n",
    "    features = (raw_features - raw_features_min)/(raw_features_max - raw_features_min)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48548549  0.47747748  0.53753754 ...,  0.47947948  0.47547548\n",
      "   0.4964965 ]\n",
      " [ 0.48348348  0.45845846  0.46046046 ...,  0.49249249  0.51051051\n",
      "   0.51751752]\n",
      " [ 0.48748749  0.54254254  0.4994995  ...,  0.48948949  0.4994995\n",
      "   0.4984985 ]\n",
      " ..., \n",
      " [ 0.48048048  0.51751752  0.63163163 ...,  0.5005005   0.52352352\n",
      "   0.48148148]\n",
      " [ 0.48448448  0.48148148  0.50550551 ...,  0.47347347  0.52752753\n",
      "   0.48548549]\n",
      " [ 0.47447447  0.49349349  0.46946947 ...,  0.48948949  0.51651652\n",
      "   0.51651652]]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "features = normalization(features)\n",
    "print(features)\n",
    "# Check that the range of each feature in the training set is in range [0, 1]\n",
    "for x in features:\n",
    "    for y in x:\n",
    "        if float(y) < float(0) and float(y) >float(1):\n",
    "            print(\"Not all features are in the range of [0,1]\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
